{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetune GAP-text2SQL for SQL Query Generation on Bank Account Fraud Dataset Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open in SageMaker Studio Lab](https://studiolab.sagemaker.aws/studiolab.svg)](https://studiolab.sagemaker.aws/import/github/vxcent/sagemaker_gap_demo/blob/main/rat-sql-gap/notebook.ipynb) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0. Install all necessary packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open up a Terminal window through Sagemaker Studio Lab for issuing the following commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conda activate default\n",
    "conda install -y python=3.7\n",
    "conda install -y -c conda-forge jsonnet openjdk\n",
    "conda install -y pytorch=1.5 cudatoolkit=10.2 -c pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the following cells to install Python dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==3.0\n",
      "  Using cached transformers-3.0.0-py3-none-any.whl (754 kB)\n",
      "Collecting jsonnet~=0.14.0\n",
      "  Using cached jsonnet-0.14.0-cp37-cp37m-linux_x86_64.whl\n",
      "Collecting nltk~=3.4\n",
      "  Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Collecting networkx~=2.2\n",
      "  Using cached networkx-2.6.3-py3-none-any.whl (1.9 MB)\n",
      "Collecting entmax\n",
      "  Using cached entmax-1.1-py3-none-any.whl (12 kB)\n",
      "Collecting pyrsistent~=0.14.9\n",
      "  Using cached pyrsistent-0.14.11-cp37-cp37m-linux_x86_64.whl\n",
      "Collecting bpemb~=0.2.11\n",
      "  Using cached bpemb-0.2.12-py3-none-any.whl (19 kB)\n",
      "Collecting stanford-corenlp~=3.9.2\n",
      "  Using cached stanford_corenlp-3.9.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting torchtext~=0.3.1\n",
      "  Using cached torchtext-0.3.1-py3-none-any.whl (62 kB)\n",
      "Collecting flask~=2.2.5\n",
      "  Using cached Flask-2.2.5-py3-none-any.whl (101 kB)\n",
      "Collecting attrs~=23.1.0\n",
      "  Using cached attrs-23.1.0-py3-none-any.whl (61 kB)\n",
      "Collecting asdl~=0.1.5\n",
      "  Using cached asdl-0.1.5-py3-none-any.whl\n",
      "Collecting protobuf==3.20.*\n",
      "  Using cached protobuf-3.20.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
      "Collecting astor~=0.8.1\n",
      "  Using cached astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
      "Collecting sacremoses\n",
      "  Using cached sacremoses-0.0.53-py3-none-any.whl\n",
      "Collecting requests\n",
      "  Using cached requests-2.31.0-py3-none-any.whl (62 kB)\n",
      "Collecting tqdm>=4.27\n",
      "  Using cached tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
      "Collecting filelock\n",
      "  Using cached filelock-3.12.2-py3-none-any.whl (10 kB)\n",
      "Collecting tokenizers==0.8.0-rc4\n",
      "  Using cached tokenizers-0.8.0rc4-cp37-cp37m-manylinux1_x86_64.whl (3.0 MB)\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.99-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Requirement already satisfied: packaging in /home/studio-lab-user/.conda/envs/default/lib/python3.7/site-packages (from transformers==3.0->-r requirements.txt (line 1)) (23.0)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15.7 MB 9.3 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting regex!=2019.12.17\n",
      "  Using cached regex-2023.6.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (755 kB)\n",
      "Collecting click\n",
      "  Using cached click-8.1.6-py3-none-any.whl (97 kB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "Collecting torch>=1.0\n",
      "  Downloading torch-1.13.1-cp37-cp37m-manylinux1_x86_64.whl (887.5 MB)\n",
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–     | 726.0 MB 83.4 MB/s eta 0:00:026B 64.0 MB/s eta 0:00:14                       | 61.7 MB 64.0 MB/s eta 0:00:13MB/s eta 0:00:10                  | 200.0 MB 78.2 MB/s eta 0:00:09ï¿½â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹                 | 403.8 MB 68.9 MB/s eta 0:00:08     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž               | 452.3 MB 63.7 MB/s eta 0:00:07â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹          | 600.0 MB 91.1 MB/s eta 0:00:04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K     |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 887.5 MB 5.7 kB/s s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six in /home/studio-lab-user/.conda/envs/default/lib/python3.7/site-packages (from pyrsistent~=0.14.9->-r requirements.txt (line 6)) (1.16.0)\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.2.0-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.1 MB)\n",
      "Collecting corenlp-protobuf>=3.8.0\n",
      "  Using cached corenlp_protobuf-3.8.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting Werkzeug>=2.2.2\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Collecting itsdangerous>=2.0\n",
      "  Using cached itsdangerous-2.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: importlib-metadata>=3.6.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.7/site-packages (from flask~=2.2.5->-r requirements.txt (line 10)) (4.11.4)\n",
      "Requirement already satisfied: Jinja2>=3.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.7/site-packages (from flask~=2.2.5->-r requirements.txt (line 10)) (3.1.2)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.7/site-packages (from importlib-metadata>=3.6.0->flask~=2.2.5->-r requirements.txt (line 10)) (3.15.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/studio-lab-user/.conda/envs/default/lib/python3.7/site-packages (from importlib-metadata>=3.6.0->flask~=2.2.5->-r requirements.txt (line 10)) (4.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/studio-lab-user/.conda/envs/default/lib/python3.7/site-packages (from Jinja2>=3.0->flask~=2.2.5->-r requirements.txt (line 10)) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/studio-lab-user/.conda/envs/default/lib/python3.7/site-packages (from requests->transformers==3.0->-r requirements.txt (line 1)) (3.4)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Using cached charset_normalizer-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
      "Collecting urllib3<3,>=1.21.1\n",
      "  Using cached urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_nvrtc_cu11-11.7.99-2-py3-none-manylinux1_x86_64.whl (21.0 MB)\n",
      "Collecting nvidia-cuda-runtime-cu11==11.7.99\n",
      "  Using cached nvidia_cuda_runtime_cu11-11.7.99-py3-none-manylinux1_x86_64.whl (849 kB)\n",
      "Collecting nvidia-cublas-cu11==11.10.3.66\n",
      "  Using cached nvidia_cublas_cu11-11.10.3.66-py3-none-manylinux1_x86_64.whl (317.1 MB)\n",
      "Collecting nvidia-cudnn-cu11==8.5.0.96\n",
      "  Using cached nvidia_cudnn_cu11-8.5.0.96-2-py3-none-manylinux1_x86_64.whl (557.1 MB)\n",
      "Requirement already satisfied: wheel in /home/studio-lab-user/.conda/envs/default/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.0->entmax->-r requirements.txt (line 5)) (0.40.0)\n",
      "Requirement already satisfied: setuptools in /home/studio-lab-user/.conda/envs/default/lib/python3.7/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch>=1.0->entmax->-r requirements.txt (line 5)) (67.6.1)\n",
      "Collecting scipy>=0.18.1\n",
      "  Using cached scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Using cached smart_open-6.3.0-py3-none-any.whl (56 kB)\n",
      "Installing collected packages: nvidia-cublas-cu11, numpy, urllib3, tqdm, smart-open, scipy, regex, protobuf, nvidia-cudnn-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, joblib, click, charset-normalizer, certifi, Werkzeug, torch, tokenizers, sentencepiece, sacremoses, requests, itsdangerous, gensim, filelock, corenlp-protobuf, transformers, torchtext, stanford-corenlp, pyrsistent, nltk, networkx, jsonnet, flask, entmax, bpemb, attrs, astor, asdl\n",
      "  Attempting uninstall: pyrsistent\n",
      "    Found existing installation: pyrsistent 0.18.1\n",
      "    Uninstalling pyrsistent-0.18.1:\n",
      "      Successfully uninstalled pyrsistent-0.18.1\n",
      "  Attempting uninstall: jsonnet\n",
      "    Found existing installation: jsonnet 0.17.0\n",
      "    Uninstalling jsonnet-0.17.0:\n",
      "      Successfully uninstalled jsonnet-0.17.0\n",
      "  Attempting uninstall: attrs\n",
      "    Found existing installation: attrs 22.2.0\n",
      "    Uninstalling attrs-22.2.0:\n",
      "      Successfully uninstalled attrs-22.2.0\n",
      "Successfully installed Werkzeug-2.2.3 asdl-0.1.5 astor-0.8.1 attrs-23.1.0 bpemb-0.2.12 certifi-2023.7.22 charset-normalizer-3.2.0 click-8.1.6 corenlp-protobuf-3.8.0 entmax-1.1 filelock-3.12.2 flask-2.2.5 gensim-4.2.0 itsdangerous-2.1.2 joblib-1.3.1 jsonnet-0.14.0 networkx-2.6.3 nltk-3.8.1 numpy-1.21.6 nvidia-cublas-cu11-11.10.3.66 nvidia-cuda-nvrtc-cu11-11.7.99 nvidia-cuda-runtime-cu11-11.7.99 nvidia-cudnn-cu11-8.5.0.96 protobuf-3.20.3 pyrsistent-0.14.11 regex-2023.6.3 requests-2.31.0 sacremoses-0.0.53 scipy-1.7.3 sentencepiece-0.1.99 smart-open-6.3.0 stanford-corenlp-3.9.2 tokenizers-0.8.0rc4 torch-1.13.1 torchtext-0.3.1 tqdm-4.65.0 transformers-3.0.0 urllib3-2.0.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/studio-lab-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /home/studio-lab-\n",
      "[nltk_data]     user/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir third_party\n",
    "wget http://nlp.stanford.edu/software/stanford-corenlp-full-2018-10-05.zip\n",
    "unzip stanford-corenlp-full-2018-10-05.zip -d third_party/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the Pretrained Model and Finetune Checkpoint (in terminal shell)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download Finetune Checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'sagemaker_gap_demo/rat-sql-gap'\n",
      "/home/studio-lab-user/sagemaker_gap_demo/rat-sql-gap\n"
     ]
    }
   ],
   "source": [
    "cd sagemaker_gap_demo/rat-sql-gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p logdir/bart_run_1/bs\\=12\\,lr\\=1.0e-04\\,bert_lr\\=1.0e-05\\,end_lr\\=0e0\\,att\\=1/\n",
    "curl https://gap-text2sql-public.s3.amazonaws.com/checkpoint-artifacts/gap-finetuned-checkpoint -o logdir/bart_run_1/bs\\=12\\,lr\\=1.0e-04\\,bert_lr\\=1.0e-05\\,end_lr\\=0e0\\,att\\=1/model_checkpoint-00041000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the Pretrained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3773467495.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_4066/3773467495.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    mkdir -p pretrained_checkpoint\u001b[0m\n\u001b[0m                                 ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "mkdir -p pretrained_checkpoint\n",
    "curl https://gap-text2sql-public.s3.amazonaws.com/checkpoint-artifacts/pretrained-checkpoint -o pretrained_checkpoint/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import _jsonnet\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seq2struct.commands.infer import Inferer\n",
    "from seq2struct.datasets.spider import SpiderItem\n",
    "from seq2struct.utils import registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The gap-run.jsonnet file includes metadata used for experiment tracking\n",
    "#### The most important field `model_config_args` includes hyperperameters used for inferences\n",
    "\n",
    "#### To learn more about the usage of the 'facebook/bart-large' model, checkout the [Huggin Face ðŸ¤— Documentation](https://huggingface.co/facebook/bart-large) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "exp_config = json.loads(\n",
    "    _jsonnet.evaluate_file(\n",
    "        \"experiments/spider-configs/gap-run.jsonnet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_config_path = exp_config[\"model_config\"]\n",
    "model_config_args = exp_config.get(\"model_config_args\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_config = json.loads(\n",
    "    _jsonnet.evaluate_file(\n",
    "        model_config_path, \n",
    "        tla_codes={'args': json.dumps(model_config_args)}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "infer_config[\"model\"][\"encoder_preproc\"][\"db_path\"] = \"data/sqlite_files/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'seq2struct.models.enc_dec.EncDecModel.Preproc'>: superfluous {'name': 'EncDec'}\n"
     ]
    }
   ],
   "source": [
    "inferer = Inferer(infer_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir = exp_config[\"logdir\"] + \"/bs=12,lr=1.0e-04,bert_lr=1.0e-05,end_lr=0e0,att=1\"\n",
    "checkpoint_step = exp_config[\"eval_steps\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logdir/bart_run_1/bs=12,lr=1.0e-04,bert_lr=1.0e-05,end_lr=0e0,att=1\n",
      "41000\n"
     ]
    }
   ],
   "source": [
    "print(model_dir)\n",
    "print(checkpoint_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING <class 'seq2struct.models.enc_dec.EncDecModel'>: superfluous {'decoder_preproc': {'grammar': {'clause_order': None, 'end_with_from': True, 'factorize_sketch': 2, 'include_literals': False, 'infer_from_conditions': True, 'name': 'spider', 'output_from': True, 'use_table_pointer': True}, 'save_path': 'data/spider-bart/nl2code-1115,output_from=true,fs=2,emb=bart,cvlink', 'use_seq_elem_rules': True}, 'encoder_preproc': {'bart_version': 'facebook/bart-large', 'compute_cv_link': True, 'compute_sc_link': True, 'db_path': 'data/sqlite_files/', 'fix_issue_16_primary_keys': True, 'include_table_name_in_column': False, 'save_path': 'data/spider-bart/nl2code-1115,output_from=true,fs=2,emb=bart,cvlink'}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0370,  0.1117,  0.1829,  ...,  0.2054,  0.0578, -0.0750],\n",
      "        [ 0.0055, -0.0049, -0.0069,  ..., -0.0030,  0.0038,  0.0087],\n",
      "        [-0.0448,  0.4604, -0.0604,  ...,  0.1073,  0.0310,  0.0477],\n",
      "        ...,\n",
      "        [-0.0138,  0.0278, -0.0467,  ...,  0.0455, -0.0265,  0.0125],\n",
      "        [-0.0043,  0.0153, -0.0567,  ...,  0.0496,  0.0108, -0.0099],\n",
      "        [ 0.0053,  0.0324, -0.0179,  ..., -0.0085,  0.0223, -0.0020]],\n",
      "       requires_grad=True)\n",
      "Updated the model with ./pretrained_checkpoint/pytorch_model.bin\n",
      "Parameter containing:\n",
      "tensor([[-0.0383,  0.1205,  0.1776,  ...,  0.1973,  0.0594, -0.0699],\n",
      "        [ 0.0046, -0.0023, -0.0084,  ..., -0.0036,  0.0047,  0.0084],\n",
      "        [-0.0460,  0.4671, -0.0650,  ...,  0.1027,  0.0256,  0.0475],\n",
      "        ...,\n",
      "        [-0.0097, -0.0304,  0.0090,  ..., -0.0029,  0.0137, -0.0164],\n",
      "        [-0.0134, -0.0092, -0.0100,  ..., -0.0006, -0.0203,  0.0194],\n",
      "        [ 0.0012,  0.0025,  0.0163,  ..., -0.0251, -0.0214, -0.0120]],\n",
      "       requires_grad=True)\n",
      "Loading model from logdir/bart_run_1/bs=12,lr=1.0e-04,bert_lr=1.0e-05,end_lr=0e0,att=1/model_checkpoint-00041000\n"
     ]
    }
   ],
   "source": [
    "model = inferer.load_model(model_dir, checkpoint_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seq2struct.datasets.spider_lib.preprocess.get_tables import dump_db_json_schema\n",
    "from seq2struct.datasets.spider import load_tables_from_schema_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_id = \"security_1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "my_schema = dump_db_json_schema(\"data/sqlite_files/{db_id}/{db_id}.sqlite\".format(db_id=db_id), db_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from seq2struct.utils.api_utils import refine_schema_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'security_1',\n",
       " 'table_names_original': ['BASE', 'VARIANT_1'],\n",
       " 'table_names': ['base', 'variant 1'],\n",
       " 'column_names_original': [(-1, '*'),\n",
       "  (0, 'FRAUD_BOOL'),\n",
       "  (0, 'INCOME'),\n",
       "  (0, 'NAME_EMAIL_SIMILARITY'),\n",
       "  (0, 'PREV_ADDRESS_MONTHS_COUNT'),\n",
       "  (0, 'CURRENT_ADDRESS_MONTHS_COUNT'),\n",
       "  (0, 'CUSTOMER_AGE'),\n",
       "  (0, 'DAYS_SINCE_REQUEST'),\n",
       "  (0, 'INTENDED_BALCON_AMOUNT'),\n",
       "  (0, 'PAYMENT_TYPE'),\n",
       "  (0, 'ZIP_COUNT_4W'),\n",
       "  (0, 'VELOCITY_6H'),\n",
       "  (0, 'VELOCITY_24H'),\n",
       "  (0, 'VELOCITY_4W'),\n",
       "  (0, 'BANK_BRANCH_COUNT_8W'),\n",
       "  (0, 'DATE_OF_BIRTH_DISTINCT_EMAILS_4W'),\n",
       "  (0, 'EMPLOYMENT_STATUS'),\n",
       "  (0, 'CREDIT_RISK_SCORE'),\n",
       "  (0, 'EMAIL_IS_FREE'),\n",
       "  (0, 'HOUSING_STATUS'),\n",
       "  (0, 'PHONE_HOME_VALID'),\n",
       "  (0, 'PHONE_MOBILE_VALID'),\n",
       "  (0, 'BANK_MONTHS_COUNT'),\n",
       "  (0, 'HAS_OTHER_CARDS'),\n",
       "  (0, 'PROPOSED_CREDIT_LIMIT'),\n",
       "  (0, 'FOREIGN_REQUEST'),\n",
       "  (0, 'SOURCE'),\n",
       "  (0, 'SESSION_LENGTH_IN_MINUTES'),\n",
       "  (0, 'DEVICE_OS'),\n",
       "  (0, 'KEEP_ALIVE_SESSION'),\n",
       "  (0, 'DEVICE_DISTINCT_EMAILS_8W'),\n",
       "  (0, 'DEVICE_FRAUD_COUNT'),\n",
       "  (0, 'MONTH'),\n",
       "  (1, 'FRAUD_BOOL'),\n",
       "  (1, 'INCOME'),\n",
       "  (1, 'NAME_EMAIL_SIMILARITY'),\n",
       "  (1, 'PREV_ADDRESS_MONTHS_COUNT'),\n",
       "  (1, 'CURRENT_ADDRESS_MONTHS_COUNT'),\n",
       "  (1, 'CUSTOMER_AGE'),\n",
       "  (1, 'DAYS_SINCE_REQUEST'),\n",
       "  (1, 'INTENDED_BALCON_AMOUNT'),\n",
       "  (1, 'PAYMENT_TYPE'),\n",
       "  (1, 'ZIP_COUNT_4W'),\n",
       "  (1, 'VELOCITY_6H'),\n",
       "  (1, 'VELOCITY_24H'),\n",
       "  (1, 'VELOCITY_4W'),\n",
       "  (1, 'BANK_BRANCH_COUNT_8W'),\n",
       "  (1, 'DATE_OF_BIRTH_DISTINCT_EMAILS_4W'),\n",
       "  (1, 'EMPLOYMENT_STATUS'),\n",
       "  (1, 'CREDIT_RISK_SCORE'),\n",
       "  (1, 'EMAIL_IS_FREE'),\n",
       "  (1, 'HOUSING_STATUS'),\n",
       "  (1, 'PHONE_HOME_VALID'),\n",
       "  (1, 'PHONE_MOBILE_VALID'),\n",
       "  (1, 'BANK_MONTHS_COUNT'),\n",
       "  (1, 'HAS_OTHER_CARDS'),\n",
       "  (1, 'PROPOSED_CREDIT_LIMIT'),\n",
       "  (1, 'FOREIGN_REQUEST'),\n",
       "  (1, 'SOURCE'),\n",
       "  (1, 'SESSION_LENGTH_IN_MINUTES'),\n",
       "  (1, 'DEVICE_OS'),\n",
       "  (1, 'KEEP_ALIVE_SESSION'),\n",
       "  (1, 'DEVICE_DISTINCT_EMAILS_8W'),\n",
       "  (1, 'DEVICE_FRAUD_COUNT'),\n",
       "  (1, 'MONTH')],\n",
       " 'column_names': [(-1, '*'),\n",
       "  (0, 'fraud bool'),\n",
       "  (0, 'income'),\n",
       "  (0, 'name email similarity'),\n",
       "  (0, 'prev address months count'),\n",
       "  (0, 'current address months count'),\n",
       "  (0, 'customer age'),\n",
       "  (0, 'days since request'),\n",
       "  (0, 'intended balcon amount'),\n",
       "  (0, 'payment type'),\n",
       "  (0, 'zip count 4w'),\n",
       "  (0, 'velocity 6h'),\n",
       "  (0, 'velocity 24h'),\n",
       "  (0, 'velocity 4w'),\n",
       "  (0, 'bank branch count 8w'),\n",
       "  (0, 'date of birth distinct emails 4w'),\n",
       "  (0, 'employment status'),\n",
       "  (0, 'credit risk score'),\n",
       "  (0, 'email is free'),\n",
       "  (0, 'housing status'),\n",
       "  (0, 'phone home valid'),\n",
       "  (0, 'phone mobile valid'),\n",
       "  (0, 'bank months count'),\n",
       "  (0, 'has other cards'),\n",
       "  (0, 'proposed credit limit'),\n",
       "  (0, 'foreign request'),\n",
       "  (0, 'source'),\n",
       "  (0, 'session length in minutes'),\n",
       "  (0, 'device os'),\n",
       "  (0, 'keep alive session'),\n",
       "  (0, 'device distinct emails 8w'),\n",
       "  (0, 'device fraud count'),\n",
       "  (0, 'month'),\n",
       "  (1, 'fraud bool'),\n",
       "  (1, 'income'),\n",
       "  (1, 'name email similarity'),\n",
       "  (1, 'prev address months count'),\n",
       "  (1, 'current address months count'),\n",
       "  (1, 'customer age'),\n",
       "  (1, 'days since request'),\n",
       "  (1, 'intended balcon amount'),\n",
       "  (1, 'payment type'),\n",
       "  (1, 'zip count 4w'),\n",
       "  (1, 'velocity 6h'),\n",
       "  (1, 'velocity 24h'),\n",
       "  (1, 'velocity 4w'),\n",
       "  (1, 'bank branch count 8w'),\n",
       "  (1, 'date of birth distinct emails 4w'),\n",
       "  (1, 'employment status'),\n",
       "  (1, 'credit risk score'),\n",
       "  (1, 'email is free'),\n",
       "  (1, 'housing status'),\n",
       "  (1, 'phone home valid'),\n",
       "  (1, 'phone mobile valid'),\n",
       "  (1, 'bank months count'),\n",
       "  (1, 'has other cards'),\n",
       "  (1, 'proposed credit limit'),\n",
       "  (1, 'foreign request'),\n",
       "  (1, 'source'),\n",
       "  (1, 'session length in minutes'),\n",
       "  (1, 'device os'),\n",
       "  (1, 'keep alive session'),\n",
       "  (1, 'device distinct emails 8w'),\n",
       "  (1, 'device fraud count'),\n",
       "  (1, 'month')],\n",
       " 'column_types': ['text',\n",
       "  'boolean',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'number',\n",
       "  'boolean',\n",
       "  'text',\n",
       "  'boolean',\n",
       "  'boolean',\n",
       "  'number',\n",
       "  'boolean',\n",
       "  'number',\n",
       "  'boolean',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'boolean',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'boolean',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'number',\n",
       "  'boolean',\n",
       "  'text',\n",
       "  'boolean',\n",
       "  'boolean',\n",
       "  'number',\n",
       "  'boolean',\n",
       "  'number',\n",
       "  'boolean',\n",
       "  'text',\n",
       "  'number',\n",
       "  'text',\n",
       "  'boolean',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number'],\n",
       " 'primary_keys': [],\n",
       " 'foreign_keys': []}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema, eval_foreign_key_maps = load_tables_from_schema_dict(my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['security_1'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "schema.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = registry.construct('dataset_infer', {\n",
    "   \"name\": \"spider\", \"schemas\": schema, \"eval_foreign_key_maps\": eval_foreign_key_maps, \n",
    "    \"db_path\": \"data/sqlite_files/\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "We're using the Stanford CoreNLP module to preprocess the schema items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _, schema in dataset.schemas.items():\n",
    "    model.preproc.enc_preproc._preprocess_schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spider_schema = dataset.schemas[db_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def infer(question):\n",
    "    data_item = SpiderItem(\n",
    "            text=None,  # intentionally None -- should be ignored when the tokenizer is set correctly\n",
    "            code=None,\n",
    "            schema=spider_schema,\n",
    "            orig_schema=spider_schema.orig,\n",
    "            orig={\"question\": question}\n",
    "        )\n",
    "    model.preproc.clear_items()\n",
    "    enc_input = model.preproc.enc_preproc.preprocess_item(data_item, None)\n",
    "    preproc_data = enc_input, None\n",
    "    with torch.no_grad():\n",
    "        output = inferer._infer_one(model, data_item, preproc_data, beam_size=1, use_heuristic=True)\n",
    "    return output[0][\"inferred_code\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT DISTINCT BASE.DEVICE_OS FROM BASE\n"
     ]
    }
   ],
   "source": [
    "code = infer(\"What type of device operating systems are used by the customer?\")\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT BASE.PAYMENT_TYPE, Count(*) FROM BASE GROUP BY BASE.PAYMENT_TYPE ORDER BY Count(*) Desc LIMIT 1\n"
     ]
    }
   ],
   "source": [
    "code = infer(\"Can you provide a breakdown of the different payment types used by customers in the database, and how many customers used each payment type in the last month?\")\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT Avg(BASE.CREDIT_RISK_SCORE), BASE.CUSTOMER_AGE FROM BASE GROUP BY BASE.CUSTOMER_AGE\n"
     ]
    }
   ],
   "source": [
    "code = infer(\"What is the average credit risk score based on age group?\")\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's see some inferences ran on other database schemas!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db_id = \"twitter_1\"\n",
    "my_schema = dump_db_json_schema(\"data/sqlite_files/{db_id}/{db_id}.sqlite\".format(db_id=db_id), db_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'db_id': 'twitter_1',\n",
       " 'table_names_original': ['follows', 'tweets', 'user_profiles'],\n",
       " 'table_names': ['follows', 'tweets', 'user profiles'],\n",
       " 'column_names_original': [(-1, '*'),\n",
       "  (0, 'f1'),\n",
       "  (0, 'f2'),\n",
       "  (1, 'id'),\n",
       "  (1, 'uid'),\n",
       "  (1, 'text'),\n",
       "  (1, 'createdate'),\n",
       "  (2, 'uid'),\n",
       "  (2, 'name'),\n",
       "  (2, 'email'),\n",
       "  (2, 'partitionid'),\n",
       "  (2, 'followers')],\n",
       " 'column_names': [(-1, '*'),\n",
       "  (0, 'f1'),\n",
       "  (0, 'f2'),\n",
       "  (1, 'id'),\n",
       "  (1, 'uid'),\n",
       "  (1, 'text'),\n",
       "  (1, 'createdate'),\n",
       "  (2, 'uid'),\n",
       "  (2, 'name'),\n",
       "  (2, 'email'),\n",
       "  (2, 'partitionid'),\n",
       "  (2, 'followers')],\n",
       " 'column_types': ['text',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'number',\n",
       "  'text',\n",
       "  'time',\n",
       "  'number',\n",
       "  'text',\n",
       "  'text',\n",
       "  'number',\n",
       "  'number'],\n",
       " 'primary_keys': [1, 3, 7],\n",
       " 'foreign_keys': [[2, 7], [1, 7], [4, 7]]}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema, eval_foreign_key_maps = load_tables_from_schema_dict(my_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = registry.construct('dataset_infer', {\n",
    "   \"name\": \"spider\", \"schemas\": schema, \"eval_foreign_key_maps\": eval_foreign_key_maps, \n",
    "    \"db_path\": \"data/sqlite_files/\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for _, schema in dataset.schemas.items():\n",
    "    model.preproc.enc_preproc._preprocess_schema(schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SELECT BASE.NAME_EMAIL_SIMILARITY, BASE.EMAIL_IS_FREE FROM BASE WHERE BASE.NAME_EMAIL_SIMILARITY LIKE 'terminal'\n"
     ]
    }
   ],
   "source": [
    "code = infer(\"Find the name and email of the user whose name contains the word '%Swift%'\")\n",
    "print(code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
